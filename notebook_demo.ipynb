{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIZOkC1Qi7j-",
        "outputId": "1a902652-5ad6-4c54-ff31-6054205b3eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake grid saved to fake_grid_0.png\n",
            "Fake grid saved to fake_grid_1.png\n",
            "Fake grid saved to fake_grid_2.png\n",
            "Fake grid saved to fake_grid_3.png\n",
            "Fake grid saved to fake_grid_4.png\n",
            "Fake grid saved to fake_grid_5.png\n",
            "Fake grid saved to fake_grid_6.png\n",
            "Fake grid saved to fake_grid_7.png\n",
            "Fake grid saved to fake_grid_8.png\n",
            "Fake grid saved to fake_grid_9.png\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# for image/model saves.\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "\n",
        "LATENT_RANGE = [0, 1]\n",
        "LATENT_SIZE = 100\n",
        "LRELU_ALPHA = 0.3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "class TConv2D:\n",
        "    def __init__(self, batch_size, input_shape, num_filters, kernel_size, stride, padding, activation, frozen=False, batch_norm=False):\n",
        "        self.frozen = frozen\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "        self.num_filters = num_filters\n",
        "        self.padding = padding\n",
        "        self.batch_size = batch_size\n",
        "        self.input_height, self.input_width, self.depth = input_shape\n",
        "        self.kernel_size, self.stride = kernel_size, stride\n",
        "        self.feature_map_width = (self.input_width - 1) * self.stride + self.kernel_size - 2 * self.padding\n",
        "        self.feature_map_height = (self.input_height - 1) * self.stride + self.kernel_size - 2 * self.padding\n",
        "\n",
        "        if not (np.modf(self.feature_map_width)[0] == 0.0 and np.modf(self.feature_map_height)[0] == 0.0):\n",
        "            raise Exception(\"Error. Feature map size must be a integer.\")\n",
        "\n",
        "        self.feature_map_width, self.feature_map_height = int(self.feature_map_width), int(self.feature_map_height)\n",
        "\n",
        "        std_dev = np.sqrt(2 / (self.kernel_size * self.kernel_size * self.depth))\n",
        "        self.W = np.random.normal(0, std_dev, (num_filters, kernel_size, kernel_size, self.depth))\n",
        "        self.B = np.zeros((num_filters,))\n",
        "        self.Z = np.zeros((batch_size, self.feature_map_height, self.feature_map_width, num_filters))\n",
        "        self.Z_unpadded = np.zeros((batch_size, self.feature_map_height + 2 * self.padding, self.feature_map_width + 2 * self.padding, num_filters))\n",
        "        self.A = np.zeros_like(self.Z)\n",
        "\n",
        "        if self.padding > 0:\n",
        "            self.dL_dZ_unpadded = np.zeros_like(self.Z_unpadded)\n",
        "        else:\n",
        "            self.dL_dZ_unpadded = None\n",
        "\n",
        "        if self.batch_norm:\n",
        "            self.scale = np.ones((num_filters,))\n",
        "            self.shift = np.zeros((num_filters,))\n",
        "            self.scale_grad = np.zeros_like(self.scale)\n",
        "            self.shift_grad = np.zeros_like(self.shift)\n",
        "            self.running_mean = np.zeros((num_filters,))\n",
        "            self.running_var = np.ones((num_filters,))\n",
        "            self.momentum = 0.99\n",
        "            self.epsilon = 1e-3\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.reshape((self.batch_size, self.input_height, self.input_width, self.depth))\n",
        "        self.input = input\n",
        "        self.Z_unpadded.fill(0)\n",
        "        for i in range(self.input_height):\n",
        "            for j in range(self.input_width):\n",
        "                start_i = i * self.stride\n",
        "                start_j = j * self.stride\n",
        "                end_i = start_i + self.kernel_size\n",
        "                end_j = start_j + self.kernel_size\n",
        "                input_pixel = self.input[:, i, j, :].reshape((self.batch_size, 1, 1, self.depth))\n",
        "                for k in range(self.num_filters):\n",
        "                    self.Z_unpadded[:, start_i:end_i, start_j:end_j, k] += np.sum(\n",
        "                        input_pixel * self.W[k], axis=-1\n",
        "                    )\n",
        "        if self.padding > 0:\n",
        "            self.Z = self.Z_unpadded[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
        "        else:\n",
        "            self.Z = self.Z_unpadded.copy()\n",
        "\n",
        "        self.Z += self.B[np.newaxis, np.newaxis, np.newaxis, :]\n",
        "\n",
        "        if self.batch_norm:\n",
        "            if not self.frozen:\n",
        "                self.batch_mean = np.mean(self.Z, axis=(0, 1, 2), keepdims=True)\n",
        "                self.batch_variance = np.var(self.Z, axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "                self.Z_normalized = (self.Z - self.batch_mean) / np.sqrt(self.batch_variance + self.epsilon)\n",
        "\n",
        "                self.Y = self.Z_normalized * self.scale + self.shift\n",
        "\n",
        "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean.squeeze()\n",
        "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_variance.squeeze()\n",
        "\n",
        "                pre_activation = self.Y\n",
        "            else:\n",
        "                Z_normalized = (self.Z - self.running_mean.reshape(1, 1, 1, -1)) / np.sqrt(self.running_var.reshape(1, 1, 1, -1) + self.epsilon)\n",
        "                Y = Z_normalized * self.scale + self.shift\n",
        "                pre_activation = Y\n",
        "        else:\n",
        "            pre_activation = self.Z\n",
        "\n",
        "        activation = self.activation.lower()\n",
        "        if activation == 'relu':\n",
        "            self.A = np.maximum(0, pre_activation)\n",
        "            self.dA_dZ = np.where(pre_activation > 0, 1, 0)\n",
        "        elif activation in ['lrelu', 'leaky_relu']:\n",
        "            self.A = np.where(pre_activation > 0, pre_activation, pre_activation * LRELU_ALPHA)\n",
        "            self.dA_dZ = np.where(pre_activation > 0, 1, LRELU_ALPHA)\n",
        "        elif activation == 'tanh':\n",
        "            self.A = np.tanh(pre_activation)\n",
        "            self.dA_dZ = 1 - np.power(self.A, 2)\n",
        "        else:\n",
        "            raise Exception(\"Error. Unknown activation function.\")\n",
        "\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, gradient, W_delta_accumulated, B_delta_accumulated):\n",
        "        if not np.ndim(gradient) == 4:\n",
        "            raise Exception(\"Gradient must have 4 dimensions (batch_size, height, width, num_filters)\")\n",
        "        dL_dZ_final = gradient * self.dA_dZ\n",
        "        if self.batch_norm:\n",
        "            dscale = np.sum(dL_dZ_final * self.Z_normalized, axis=(0, 1, 2))\n",
        "            dshift = np.sum(dL_dZ_final, axis=(0, 1, 2))\n",
        "\n",
        "            if not self.frozen:\n",
        "                self.scale_grad += dscale\n",
        "                self.shift_grad += dshift\n",
        "\n",
        "            dZ_normalized = dL_dZ_final * self.scale\n",
        "            dvariance = np.sum(dZ_normalized * (self.Z - self.batch_mean) * -0.5 * np.power(self.batch_variance + self.epsilon, -1.5), axis=(0,1,2), keepdims=True)\n",
        "            dmean = (np.sum(dZ_normalized * -1 / np.sqrt(self.batch_variance + self.epsilon), axis=(0,1,2), keepdims=True) +\n",
        "                     dvariance * np.mean(-2 * (self.Z - self.batch_mean), axis=(0,1,2), keepdims=True))\n",
        "            N = self.batch_size * self.feature_map_height * self.feature_map_width\n",
        "            dZ = (dZ_normalized / np.sqrt(self.batch_variance + self.epsilon)) + \\\n",
        "                 (dvariance * 2 * (self.Z - self.batch_mean) / N) + \\\n",
        "                 (dmean / N)\n",
        "\n",
        "            dL_dZ_final = dZ\n",
        "\n",
        "        if self.padding > 0:\n",
        "            self.dL_dZ_unpadded.fill(0)\n",
        "            self.dL_dZ_unpadded[:, self.padding:-self.padding, self.padding:-self.padding, :] = dL_dZ_final\n",
        "            dL_dZ_unpadded = self.dL_dZ_unpadded\n",
        "        else:\n",
        "            dL_dZ_unpadded = dL_dZ_final\n",
        "        if not self.frozen:\n",
        "            B_delta_accumulated += np.sum(dL_dZ_unpadded, axis=(0,1,2))\n",
        "\n",
        "        dX = np.zeros_like(self.input)\n",
        "\n",
        "        for i in range(self.input_height):\n",
        "            for j in range(self.input_width):\n",
        "                start_i = i * self.stride\n",
        "                start_j = j * self.stride\n",
        "                end_i = start_i + self.kernel_size\n",
        "                end_j = start_j + self.kernel_size\n",
        "                for k in range(self.num_filters):\n",
        "                    dZ_region = dL_dZ_unpadded[:, start_i:end_i, start_j:end_j, k]\n",
        "\n",
        "                    if not self.frozen:\n",
        "                        input_pixel = self.input[:, i, j, :]\n",
        "                        input_pixel_expanded = input_pixel[:, np.newaxis, np.newaxis, :]\n",
        "\n",
        "                        dW = np.sum(dZ_region[:, :, :, np.newaxis] * input_pixel_expanded, axis=(0, 1, 2))\n",
        "                        W_delta_accumulated[k] += dW\n",
        "\n",
        "                    dX[:, i, j, :] += np.sum(dZ_region[:, :, :, np.newaxis] * self.W[k], axis=(1, 2))\n",
        "\n",
        "        return dX\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, batch_size, input_shape, num_neurons, activation, frozen=False, batch_norm=False):\n",
        "        self.frozen = frozen\n",
        "        self.batch_norm = batch_norm\n",
        "        self.batch_size = batch_size\n",
        "        self.input_shape = input_shape\n",
        "        self.input_size = np.prod(input_shape)\n",
        "        self.activation = activation\n",
        "        self.num_neurons = num_neurons\n",
        "        self.W = np.random.randn(self.input_size, self.num_neurons) * np.sqrt(1 / self.input_size)\n",
        "        self.B = np.zeros((num_neurons))\n",
        "\n",
        "        self.Z = np.zeros((batch_size, num_neurons))\n",
        "        self.A = np.zeros_like(self.Z)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            self.Z_normalized = np.zeros_like(self.Z)\n",
        "            self.Y = np.zeros_like(self.Z)\n",
        "            self.scale = np.ones((num_neurons,))\n",
        "            self.shift = np.zeros((num_neurons,))\n",
        "            self.scale_grad = np.zeros_like(self.scale)\n",
        "            self.shift_grad = np.zeros_like(self.shift)\n",
        "            self.running_mean = np.zeros((num_neurons,))\n",
        "            self.running_var = np.ones((num_neurons,))\n",
        "            self.momentum = 0.99\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.X_flattened = input.reshape(self.batch_size, -1)\n",
        "        self.Z = self.X_flattened.dot(self.W) + self.B\n",
        "\n",
        "        if self.batch_norm:\n",
        "            if not self.frozen:\n",
        "                # Compute mean and variance per feature\n",
        "                self.batch_mean = np.mean(self.Z, axis=0)\n",
        "                self.batch_variance = np.var(self.Z, axis=0)\n",
        "                # Normalize Z\n",
        "                self.Z_normalized = (self.Z - self.batch_mean) / np.sqrt(self.batch_variance + 1e-3)\n",
        "                # Scale and shift\n",
        "                self.Y = self.Z_normalized * self.scale + self.shift\n",
        "                # Update running estimates\n",
        "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
        "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_variance\n",
        "                pre_activation = self.Y\n",
        "            else:\n",
        "                # Use running mean and variance for normalization\n",
        "                Z_normalized = (self.Z - self.running_mean) / np.sqrt(self.running_var + 1e-3)\n",
        "                Y = Z_normalized * self.scale + self.shift\n",
        "                pre_activation = Y\n",
        "        else:\n",
        "            pre_activation = self.Z\n",
        "\n",
        "        # Activation\n",
        "        match self.activation.lower():\n",
        "            case 'linear':\n",
        "                self.A = pre_activation\n",
        "                self.dA_dZ = np.ones_like(pre_activation)\n",
        "            case 'relu':\n",
        "                self.A = np.maximum(0, pre_activation)\n",
        "                self.dA_dZ = np.where(pre_activation > 0, 1, 0)\n",
        "            case 'lrelu' | 'leaky_relu':\n",
        "                self.A = np.where(pre_activation > 0, pre_activation, pre_activation * LRELU_ALPHA)\n",
        "                self.dA_dZ = np.where(pre_activation > 0, 1, LRELU_ALPHA)\n",
        "            case 'sigmoid':\n",
        "                sigmoid = 1 / (1 + np.exp(-pre_activation))\n",
        "                self.A = sigmoid\n",
        "                self.dA_dZ = sigmoid * (1 - sigmoid)\n",
        "            case _:\n",
        "                raise Exception(\"Error. Unknown activation function.\")\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, gradient, W_delta_accumulated, B_delta_accumulated):\n",
        "        if not np.shape(gradient) == 2:\n",
        "            gradient = gradient.reshape((self.batch_size, -1))\n",
        "\n",
        "        dA_dZ = self.dA_dZ\n",
        "        dL_dZ = gradient * dA_dZ\n",
        "\n",
        "        if self.batch_norm:\n",
        "            dL_dscale = np.sum(dL_dZ * self.Z_normalized, axis=0)\n",
        "            dL_dshift = np.sum(dL_dZ, axis=0)\n",
        "\n",
        "            if not self.frozen:\n",
        "                self.scale_grad += dL_dscale\n",
        "                self.shift_grad += dL_dshift\n",
        "\n",
        "            dL_dZ_normalized = dL_dZ * self.scale\n",
        "            dL_dvar = np.sum(dL_dZ_normalized * (self.Z - self.batch_mean) * -0.5 * (self.batch_variance + 1e-3)**(-1.5), axis=0)\n",
        "            dL_dmean = np.sum(dL_dZ_normalized * -1 / np.sqrt(self.batch_variance + 1e-3), axis=0) + \\\n",
        "                       dL_dvar * np.mean(-2 * (self.Z - self.batch_mean), axis=0)\n",
        "            dL_dZ = (dL_dZ_normalized / np.sqrt(self.batch_variance + 1e-3)) + \\\n",
        "                    (dL_dvar * 2 * (self.Z - self.batch_mean) / self.batch_size) + \\\n",
        "                    (dL_dmean / self.batch_size)\n",
        "\n",
        "        dL_dW = self.X_flattened.T.dot(dL_dZ)\n",
        "        dL_dB = np.sum(dL_dZ, axis=0)\n",
        "\n",
        "        if not self.frozen:\n",
        "            W_delta_accumulated += dL_dW\n",
        "            B_delta_accumulated += dL_dB\n",
        "\n",
        "        updated_gradient = dL_dZ.dot(self.W.T)\n",
        "\n",
        "        return updated_gradient.reshape((self.batch_size,) + self.input_shape)\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, batch_size):\n",
        "        self.layers = [\n",
        "            Dense(batch_size=batch_size, input_shape=(1, 1, 100), num_neurons=8 * 8 * 256, activation=\"lrelu\", batch_norm=True),\n",
        "            TConv2D(batch_size=batch_size, input_shape=(8, 8, 256), num_filters=128, kernel_size=4, stride=2, padding=1, activation=\"lrelu\", batch_norm=True),\n",
        "            TConv2D(batch_size=batch_size, input_shape=(16, 16, 128), num_filters=64, kernel_size=4, stride=2, padding=1, activation=\"lrelu\", batch_norm=True),\n",
        "            TConv2D(batch_size=batch_size, input_shape=(32, 32, 64), num_filters=3, kernel_size=4, stride=2, padding=1, activation=\"tanh\")\n",
        "        ]\n",
        "\n",
        "        self.W_deltas = [np.zeros_like(layer.W) if hasattr(layer, 'W') else None for layer in self.layers]\n",
        "        self.B_deltas = [np.zeros_like(layer.B) if hasattr(layer, 'B') else None for layer in self.layers]\n",
        "\n",
        "    def applyDeltas(self, learning_rate):\n",
        "        for layer_index in range(len(self.layers)):\n",
        "            if hasattr(self.layers[layer_index], 'W'):\n",
        "                self.layers[layer_index].W -= learning_rate * self.W_deltas[layer_index]\n",
        "                self.layers[layer_index].B -= learning_rate * self.B_deltas[layer_index]\n",
        "    def resetDeltas(self):\n",
        "        for layer_index in range(len(self.layers)):\n",
        "            if hasattr(self.layers[layer_index], 'W'):\n",
        "                self.W_deltas[layer_index].fill(0)\n",
        "                self.B_deltas[layer_index].fill(0)\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    def backward(self, gradient):\n",
        "        current_gradient = gradient\n",
        "        for layer_index in range(len(self.layers) -1, -1, -1):\n",
        "            current_gradient = self.layers[layer_index].backward(current_gradient, self.W_deltas[layer_index], self.B_deltas[layer_index])\n",
        "        return current_gradient\n",
        "\n",
        "    def state_dict(self):\n",
        "        state = {}\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if hasattr(self.layers[i], 'W'):\n",
        "                state[f\"layer_{i}_W\"] = layer.W\n",
        "                state[f\"layer_{i}_B\"] = layer.B\n",
        "            if hasattr(self.layers[i], 'scale'):\n",
        "                state[f\"layer_{i}_scale\"] = layer.scale\n",
        "                state[f\"layer_{i}_shift\"] = layer.shift\n",
        "                state[f\"layer_{i}_running_mean\"] = layer.running_mean\n",
        "                state[f\"layer_{i}_running_var\"] = layer.running_var\n",
        "        return state\n",
        "\n",
        "    def load_state_dict(self, state):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if hasattr(self.layers[i], 'W'):\n",
        "                layer.W = state[f\"layer_{i}_W\"]\n",
        "                layer.B = state[f\"layer_{i}_B\"]\n",
        "            if hasattr(self.layers[i], 'scale'):\n",
        "                layer.scale = state[f\"layer_{i}_scale\"]\n",
        "                layer.shift = state[f\"layer_{i}_shift\"]\n",
        "                layer.running_mean = state[f\"layer_{i}_running_mean\"]\n",
        "                layer.running_var = state[f\"layer_{i}_running_var\"]\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "generator = Generator(BATCH_SIZE)\n",
        "\n",
        "\n",
        "with open(f\"trained_generator.pkl\", \"rb\") as g_state_file:\n",
        "    generator.load_state_dict(pickle.load(g_state_file))\n",
        "\n",
        "def save_image_grid(images, image_width, num_cols, output_path):\n",
        "    canvas = Image.new(\"RGBA\", (image_width * num_cols, image_width * num_cols))\n",
        "    x = 0\n",
        "    y = 0\n",
        "    canvas_size = image_width * num_cols\n",
        "    for image_idx in range(len(images)):\n",
        "        image =  Image.fromarray(((images[image_idx] + 1) * 127.5).clip(0, 255).astype(np.uint8))\n",
        "        canvas.paste(image, (x, y))\n",
        "        x += image_width\n",
        "        if x >= canvas_size:\n",
        "            x = 0\n",
        "            y += image_width\n",
        "    print(f\"Fake grid saved to {output_path}\")\n",
        "    canvas.save(output_path)\n",
        "\n",
        "grids_to_generate = 10\n",
        "for _ in range(grids_to_generate):\n",
        "    latent_vector = np.random.normal(0, 1, (BATCH_SIZE, 1, 1, LATENT_SIZE))\n",
        "    fake_batch = generator.forward(latent_vector)\n",
        "    save_image_grid(fake_batch, len(fake_batch[0]), 4, f\"fake_grid_{_}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B62Jrc4_j7fV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}